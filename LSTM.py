# âš™ï¸ 0. ì„¤ì¹˜ ë° ì„í¬íŠ¸
import os
import json
import numpy as np
from glob import glob
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict
import re
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# âœ… ë‹¨ì–´ ID â†’ ì˜ë¯¸ ë§¤í•‘
WORD_LABELS = {
    "1506": "Miss",
    "1511": "Fishing",
    "1517": "Crybaby",
    "1518": "Sister"
}
HIDDEN_DIM = 20

from collections import Counter

def compute_class_weights(y_labels, num_classes):
    """ë ˆì´ë¸” ë²¡í„°ì—ì„œ í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì—¬ CrossEntropyLossì— ì‚¬ìš©"""
    counts = Counter(y_labels)
    total = sum(counts.values())
    weights = [total / counts.get(i, 1) for i in range(num_classes)]  # count=0 íšŒí”¼ ìœ„í•´ +1 ì•ˆí•¨
    weights = torch.tensor(weights, dtype=torch.float32)
    weights = weights / weights.sum()  # ì •ê·œí™” (ì„ íƒ ì‚¬í•­)
    return weights

def interpolate_sequence(seq, target_len=120):
    """
    seq: numpy array of shape (T, 274) where T is the number of frames
    target_len: desired sequence length (default 120)
    """
    current_len = len(seq)
    if current_len == target_len:
        return seq

    x_old = np.linspace(0, 1, current_len)
    x_new = np.linspace(0, 1, target_len)
    
    # ì„ í˜• ë³´ê°„
    interpolated_seq = np.zeros((target_len, seq.shape[1]))
    for i in range(seq.shape[1]):
        interpolated_seq[:, i] = np.interp(x_new, x_old, seq[:, i])

    return interpolated_seq


# ğŸš€ 1. ì¥ì¹˜ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ“‚ 2. JSON íŒŒì‹± í•¨ìˆ˜ ì •ì˜ (OpenPose 2D keypoint ì „ìš©)
def load_keypoint_json(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)

    keypoint_fields = ['pose_keypoints_2d', 'hand_left_keypoints_2d', 'hand_right_keypoints_2d']
    keypoints = []
    for field in keypoint_fields:
        raw = data['people'][field]
        arr = np.array(raw).reshape(-1, 3)[:, :2]  # (N, 2)
        keypoints.append(arr)

    # === ğŸ¯ ìƒëŒ€ì¢Œí‘œí™” ===
    pose = keypoints[0]
    ref_point = pose[0]  # ì½”ë¥¼ ê¸°ì¤€
    keypoints = [kp - ref_point for kp in keypoints]

    # === ğŸ“ ìŠ¤ì¼€ì¼ ì •ê·œí™” ===
    all_coords = np.concatenate(keypoints, axis=0)
    std = np.std(all_coords, axis=0) + 1e-6
    keypoints = [(kp) / std for kp in keypoints]

    # flatten
    full_frame = np.concatenate(keypoints, axis=0)  # (137, 2)
    return full_frame.flatten()  # (274,)


# ğŸ“ 3. í´ë” êµ¬ì¡°: NIA_SL_WORD{word}_REAL{person}_{angle}
def collect_all_sample_paths(data_root, sequence_len=50):
    pattern = r'NIA_SL_WORD(\d+)_REAL(\d+)_([A-Z])'
    all_sequences, all_labels = [], []
    label_dict = {}
    label_id = 0

    folder_paths = sorted(glob(os.path.join(data_root, 'NIA_SL_WORD*')))

    for folder in folder_paths:
        folder_name = os.path.basename(folder)
        match = re.match(pattern, folder_name)
        if not match:
            continue
        word_id, person_id, angle = match.groups()
        if word_id not in label_dict:
            label_dict[word_id] = label_id
            label_id += 1

        json_files = sorted(glob(os.path.join(folder, '*.json')))
        if len(json_files) >= sequence_len:
            loaded_seq = [load_keypoint_json(f) for f in json_files]
            interp_seq = interpolate_sequence(np.array(loaded_seq), target_len=120)
            all_sequences.append(interp_seq)
            all_labels.append(label_dict[word_id])

    # label_dictëŠ” {"1506": 0, "1511": 1, ...}
    import json

    id_to_label = {v: WORD_LABELS[k] for k, v in label_dict.items()}
    with open("label_mapping.json", "w", encoding="utf-8") as f:
        json.dump(id_to_label, f, ensure_ascii=False, indent=2)

    # âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë‹¨ì–´ëª… ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥
    label_names = [WORD_LABELS[wid] for wid in sorted(label_dict.keys())]
    return np.array(all_sequences), np.array(all_labels), label_names

# ğŸ§º 4. PyTorch Dataset + DataLoader
class SignKeypointDataset(Dataset):
    def __init__(self, sequences, labels):
        self.data = sequences
        self.labels = labels

    def __getitem__(self, idx):
        x = torch.tensor(self.data[idx], dtype=torch.float32)
        y = torch.tensor(self.labels[idx], dtype=torch.long)
        return x, y

    def __len__(self):
        return len(self.data)
    
# ğŸ§  5. LSTM ë¶„ë¥˜ê¸° ëª¨ë¸ ì •ì˜
class LSTMClassifier(nn.Module):
    def __init__(self, input_dim=134, hidden_dim=HIDDEN_DIM, num_layers=1, num_classes=4):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        _, (hn, _) = self.lstm(x)
        return self.fc(hn[-1])

# ğŸ‹ï¸â€â™‚ï¸ 6. í•™ìŠµ ë£¨í”„

def train(model, dataloader, optimizer, criterion, device, epochs=20):
    model.to(device)
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        correct, total = 0, 0

        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            loss = criterion(pred, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            correct += (pred.argmax(dim=1) == y).sum().item()
            total += y.size(0)

        acc = correct / total
        print(f"[Epoch {epoch+1}] Loss: {total_loss / len(dataloader):.4f} | Acc: {acc:.4f}")

# ğŸ“ˆ 7. í‰ê°€ í•¨ìˆ˜ + í˜¼ë™ í–‰ë ¬ ì¶œë ¥
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

def evaluate(model, dataset, label_names):
    model.eval()
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
    preds, trues = [], []

    with torch.no_grad():
        for x, y in dataloader:
            x = x.to(device)
            out = model(x)
            pred = out.argmax(dim=1).cpu().item()
            preds.append(pred)
            trues.append(y.item())

    num_classes = len(label_names)
    label_indices = list(range(num_classes))

    # âœ… labels ì¸ì ëª…ì‹œ
    print("\nğŸ“Š Classification Report:")
    print(classification_report(
        trues, preds,
        labels=label_indices,
        target_names=label_names,
        digits=4,
        zero_division=0  # 0ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ ì˜¤ë¥˜ ë°©ì§€
    ))

    # ğŸ“‰ Confusion Matrix ì¶œë ¥
    cm = confusion_matrix(trues, preds, labels=label_indices)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap='Blues', xticks_rotation=45)
    plt.title("Confusion Matrix")
    plt.show()


# ğŸš€ 8. ì‹¤í–‰ ì½”ë“œ
if __name__ == '__main__':
    X, y, label_names = collect_all_sample_paths("data_train")
    dataset = SignKeypointDataset(X, y)
    X_val, y_val, label_names = collect_all_sample_paths("data_val")
    dataset_eval = SignKeypointDataset(X_val, y_val)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    model = LSTMClassifier(input_dim=134, hidden_dim=HIDDEN_DIM, num_layers=1, num_classes=len(label_names))

    # ğŸ”§ ìë™ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    weights = compute_class_weights(y, num_classes=len(label_names))
    criterion = nn.CrossEntropyLoss(weight=weights.to(device))
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # le-3 

    # ğŸš€ í•™ìŠµ
    train(model, dataloader, optimizer, criterion, device, epochs=50)
    from collections import Counter
    print("ì „ì²´ ìƒ˜í”Œ ìˆ˜:", len(y))
    print("í´ë˜ìŠ¤ë³„ ê°œìˆ˜:", Counter(y))

    # í‰ê°€ìš© ë°ì´í„°ì…‹ ì•ˆì— í´ë˜ìŠ¤ë³„ ê°œìˆ˜ í™•ì¸
    evaluate_labels = [label for _, label in DataLoader(dataset, batch_size=1)]
    print("evaluateìš© í´ë˜ìŠ¤ ë¶„í¬:", Counter([y.item() for y in evaluate_labels]))

    # ğŸ’¾ ì €ì¥ ë° í‰ê°€
    torch.save(model.state_dict(), "best_model.pt")
    evaluate(model, dataset_eval, label_names)


